{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73620b4a",
   "metadata": {},
   "source": [
    "# Chargement du jeu de données traité\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bc8876a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "label",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "airline_sentiment_confidence",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "negativereason",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "negativereason_confidence",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "airline",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "retweet_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "tweet_coord",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "tweet_created",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "tweet_location",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "user_timezone",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "text_clean",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "1ab070ac-b45c-4933-bd5f-58c348502468",
       "rows": [
        [
         "0",
         "5.70306e+17",
         "neutral",
         "1.0",
         null,
         null,
         "Virgin America",
         "cairdin",
         "0",
         "@VirginAmerica What @dhepburn said.",
         null,
         "2/24/2015 11:35",
         null,
         "Eastern Time (US & Canada)",
         "What said."
        ],
        [
         "1",
         "5.70301e+17",
         "positive",
         "0.3486",
         null,
         "0.0",
         "Virgin America",
         "jnardino",
         "0",
         "@VirginAmerica plus you've added commercials to the experience... tacky.",
         null,
         "2/24/2015 11:15",
         null,
         "Pacific Time (US & Canada)",
         "plus you've added commercials to the experience... tacky."
        ],
        [
         "2",
         "5.70301e+17",
         "neutral",
         "0.6837",
         null,
         null,
         "Virgin America",
         "yvonnalynn",
         "0",
         "@VirginAmerica I didn't today... Must mean I need to take another trip!",
         null,
         "2/24/2015 11:15",
         "Lets Play",
         "Central Time (US & Canada)",
         "I didn't today... Must mean I need to take another trip!"
        ],
        [
         "3",
         "5.70301e+17",
         "negative",
         "1.0",
         "Bad Flight",
         "0.7033",
         "Virgin America",
         "jnardino",
         "0",
         "@VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse",
         null,
         "2/24/2015 11:15",
         null,
         "Pacific Time (US & Canada)",
         "it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse"
        ],
        [
         "4",
         "5.70301e+17",
         "negative",
         "1.0",
         "Can't Tell",
         "1.0",
         "Virgin America",
         "jnardino",
         "0",
         "@VirginAmerica and it's a really big bad thing about it",
         null,
         "2/24/2015 11:14",
         null,
         "Pacific Time (US & Canada)",
         "and it's a really big bad thing about it"
        ]
       ],
       "shape": {
        "columns": 14,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>name</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.70306e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2/24/2015 11:35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "      <td>What said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.70301e+17</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2/24/2015 11:15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "      <td>plus you've added commercials to the experienc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.70301e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2/24/2015 11:15</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "      <td>I didn't today... Must mean I need to take ano...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.70301e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2/24/2015 11:15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "      <td>it's really aggressive to blast obnoxious \"ent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.70301e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2/24/2015 11:14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "      <td>and it's a really big bad thing about it</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id     label  airline_sentiment_confidence negativereason  \\\n",
       "0  5.70306e+17   neutral                        1.0000            NaN   \n",
       "1  5.70301e+17  positive                        0.3486            NaN   \n",
       "2  5.70301e+17   neutral                        0.6837            NaN   \n",
       "3  5.70301e+17  negative                        1.0000     Bad Flight   \n",
       "4  5.70301e+17  negative                        1.0000     Can't Tell   \n",
       "\n",
       "   negativereason_confidence         airline        name  retweet_count  \\\n",
       "0                        NaN  Virgin America     cairdin              0   \n",
       "1                     0.0000  Virgin America    jnardino              0   \n",
       "2                        NaN  Virgin America  yvonnalynn              0   \n",
       "3                     0.7033  Virgin America    jnardino              0   \n",
       "4                     1.0000  Virgin America    jnardino              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "     tweet_created tweet_location               user_timezone  \\\n",
       "0  2/24/2015 11:35            NaN  Eastern Time (US & Canada)   \n",
       "1  2/24/2015 11:15            NaN  Pacific Time (US & Canada)   \n",
       "2  2/24/2015 11:15      Lets Play  Central Time (US & Canada)   \n",
       "3  2/24/2015 11:15            NaN  Pacific Time (US & Canada)   \n",
       "4  2/24/2015 11:14            NaN  Pacific Time (US & Canada)   \n",
       "\n",
       "                                          text_clean  \n",
       "0                                         What said.  \n",
       "1  plus you've added commercials to the experienc...  \n",
       "2  I didn't today... Must mean I need to take ano...  \n",
       "3  it's really aggressive to blast obnoxious \"ent...  \n",
       "4           and it's a really big bad thing about it  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "data_path=\"/opt/airflow/data/raw/us_airline_sentiment_raw.csv\"\n",
    "df=pd.read_csv(data_path)\n",
    "df[\"text_clean\"] = df[\"text\"].str.replace(r'@[^\\s]+', '', regex=True)\n",
    "df[\"text_clean\"] = df[\"text_clean\"].fillna(\"\").str.strip()\n",
    "df[\"text_clean\"] = df[\"text_clean\"].str.replace(r\"\\s+\", \" \", regex=True)\n",
    "df.rename(columns={'tweet_id': 'id', 'airline_sentiment': 'label'}, inplace=True)\n",
    "\n",
    "df[\"id\"]=df[\"id\"].astype(str)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2d2e2b",
   "metadata": {},
   "source": [
    "Remarque — Vérification du chargement\n",
    "Si la table s'affiche, le jeu de données traité est disponible et prêt pour l'encodage. Sinon, vérifiez le chemin du fichier ou exécutez d'abord l'étape de nettoyage (`03_text_cleaning.ipynb`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff32f9d",
   "metadata": {},
   "source": [
    "# Split avant toute augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5e4ff8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"id\", \"text_clean\"]]\n",
    "y = df[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_df = pd.concat([X_train, y_train], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56c336b",
   "metadata": {},
   "source": [
    "## Augmenter UNIQUEMENT les classes minoritaires du train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ad9f0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_df = train_df[train_df['label'] == 'negative']\n",
    "neutral_df  = train_df[train_df['label'] == 'neutral']\n",
    "positive_df = train_df[train_df['label'] == 'positive']\n",
    "\n",
    "max_count = len(negative_df)\n",
    "augmented_data = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a73bae",
   "metadata": {},
   "source": [
    "# Neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56e9415e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4864\n"
     ]
    }
   ],
   "source": [
    "neutral_samples = neutral_df.sample(\n",
    "    n=max_count - len(neutral_df),\n",
    "    replace=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "for row in neutral_samples.itertuples():\n",
    "    try:\n",
    "        aug_text = aug_synonym.augment(row.text_clean)\n",
    "        augmented_data.append({\n",
    "            \"text_clean\": aug_text,\n",
    "            \"label\": row.label\n",
    "        })\n",
    "    except:\n",
    "        pass\n",
    "print(len(neutral_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093c6d09",
   "metadata": {},
   "source": [
    "# Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff0b7f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_data = []\n",
    "\n",
    "positive_samples = positive_df.sample(\n",
    "    n=max_count - len(positive_df),\n",
    "    replace=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "for row in positive_samples.itertuples():\n",
    "    try:\n",
    "        aug_text = aug_synonym.augment(row.text_clean)\n",
    "        augmented_data.append({\n",
    "            \"text_clean\": aug_text,\n",
    "            \"label\": row.label\n",
    "        })\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c133201",
   "metadata": {},
   "source": [
    "# Reconstituer le TRAIN final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc78a649",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_df = pd.DataFrame(augmented_data)\n",
    "\n",
    "train_df_final = pd.concat(\n",
    "    [train_df[[\"text_clean\", \"label\"]], augmented_df],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6b41ac",
   "metadata": {},
   "source": [
    "# Génération des embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c2d444",
   "metadata": {},
   "source": [
    "Nous avons choisi le modèle intfloat/e5-large-v2 car il génère des embeddings de qualité pour capturer la similarité sémantique entre phrases, ce qui est idéal pour notre analyse de texte.\n",
    "Il est également léger et rapide, ce qui permet de traiter de gros volumes de données sans trop de ressources.\n",
    "Ainsi, il représente un bon compromis entre précision et performance pour notre projet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ce0316",
   "metadata": {},
   "source": [
    "## Encoder uniquement le train augmenté"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cac8076c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Encoding train embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 183/183 [14:42<00:00,  4.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train embeddings done!\n",
      "Encoding test embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 46/46 [03:48<00:00,  4.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test embeddings done!\n",
      "Embeddings and metadata saved.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model_name=\"intfloat/e5-large-v2\"\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "train_embedding_path=\"/opt/airflow/data/embeddings/train_embedding.npy\"\n",
    "test_embedding_path=\"/opt/airflow/data/embeddings/test_embedding.npy\"\n",
    "train_metadata_path=\"/opt/airflow/data/metadata/train_metadata.csv\"\n",
    "test_metadata_path=\"/opt/airflow/data/metadata/test_metadata.csv\"\n",
    "\n",
    "\n",
    "\n",
    "model=SentenceTransformer(model_name,device)\n",
    "print(\"Encoding train embeddings...\")\n",
    "\n",
    "train_embedding = model.encode(\n",
    "    train_df_final[\"text_clean\"].tolist(),\n",
    "    batch_size=64,\n",
    "    convert_to_numpy=True,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Train embeddings done!\")\n",
    "\n",
    "print(\"Encoding test embeddings...\")\n",
    "\n",
    "test_embedding = model.encode(\n",
    "    X_test[\"text_clean\"].tolist(),\n",
    "    batch_size=64,\n",
    "    convert_to_numpy=True,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "print(\"Test embeddings done!\")\n",
    "\n",
    "X_train[\"id\"] = X_train[\"id\"].astype(str) + \"_\" + X_train[\"id\"].index.astype(str)\n",
    "X_test[\"id\"] = X_test[\"id\"].astype(str) + \"_\" + X_test[\"id\"].index.astype(str)\n",
    "\n",
    "train_metadata=pd.DataFrame({\n",
    "    \"id\":X_train[\"id\"].tolist(),\n",
    "    \"label\":y_train.to_numpy()\n",
    "})\n",
    "\n",
    "test_metadata=pd.DataFrame({\n",
    "    \"id\":X_test[\"id\"].tolist(),\n",
    "    \"label\":y_test.to_numpy()\n",
    "})\n",
    "\n",
    "train_metadata.to_csv(train_metadata_path,index=False)\n",
    "test_metadata.to_csv(test_metadata_path,index=False)\n",
    "\n",
    "np.save(train_embedding_path,train_embedding)\n",
    "np.save(test_embedding_path,test_embedding)\n",
    "\n",
    "print(\"Embeddings and metadata saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a33e6ea",
   "metadata": {},
   "source": [
    "- `train_embedding.npy` et `test_embedding.npy` contiennent les vecteurs d'embeddings au format NumPy.\n",
    "- `train_metadata.csv` et `test_metadata.csv` contiennent les `id` et `label` associés aux embeddings.\n",
    "Ces fichiers servent pour l'entraînement du classifieur, l'évaluation et l'indexation dans une base vectorielle.\n",
    "Conservez-les dans `data/embeddings` et `data/metadata` pour les étapes suivantes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193eb04d",
   "metadata": {},
   "source": [
    "#  Initialisation de la base vectorielle (Chroma)\n",
    "La création d'un client Chroma persistant pointant vers `data/chroma_db`. Cela permet d'indexer et de rechercher des embeddings localement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "731c8938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dcb484a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "client=chromadb.PersistentClient(path='/opt/airflow/data/chroma_dataBase')\n",
    "train_collection=client.create_collection(\"avis_train\",get_or_create=True)\n",
    "test_colletion=client.get_or_create_collection(name=\"avis_test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19419f25",
   "metadata": {},
   "source": [
    "# Indexation des embeddings — train\n",
    "Remarque : la boucle suivante ajoute les embeddings d'entraînement par lots dans la collection `avis_train`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d226d602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added train batch 0 to 1000\n",
      "Added train batch 1000 to 2000\n",
      "Added train batch 2000 to 3000\n",
      "Added train batch 3000 to 4000\n",
      "Added train batch 4000 to 5000\n",
      "Added train batch 5000 to 6000\n",
      "Added train batch 6000 to 7000\n",
      "Added train batch 7000 to 8000\n",
      "Added train batch 8000 to 9000\n",
      "Added train batch 9000 to 10000\n",
      "Added train batch 10000 to 11000\n",
      "Added train batch 11000 to 11712\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "metadatas_full = [{\"label\": i, \"split\":\"train\"} for i in train_metadata[\"label\"].to_numpy()]\n",
    "n = len(train_metadata[\"id\"])\n",
    "\n",
    "if \"avis_train\" in client.list_collections():\n",
    "    client.delete_collection(\"avis_train\")\n",
    "\n",
    "train_collection = client.get_or_create_collection(\"avis_train\")\n",
    "\n",
    "for i in range(0, n, batch_size):  \n",
    "    ids = train_metadata[\"id\"][i:i+batch_size].tolist()\n",
    "    metadatas = metadatas_full[i:i+batch_size]\n",
    "    documents = X_train['text_clean'][i:i+batch_size].tolist()  \n",
    "    batch_embeddings = train_embedding[i:i+batch_size].tolist()  \n",
    "\n",
    "    train_collection.add(\n",
    "        ids=ids,\n",
    "        embeddings=batch_embeddings,\n",
    "        metadatas=metadatas,\n",
    "        documents=documents  \n",
    "    )\n",
    "    print(f\"Added train batch {i} to {i+len(ids)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79593dac",
   "metadata": {},
   "source": [
    "Après exécution, la collection `avis_train` contiendra les embeddings d'entraînement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1fc7a6",
   "metadata": {},
   "source": [
    "# Indexation des embeddings — test\n",
    "La boucle suivante ajoute les embeddings de test par lots dans la collection `avis_test`. Les impressions indiquent la progression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0155f7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added test batch 0 to 1000\n",
      "Added test batch 1000 to 2000\n",
      "Added test batch 2000 to 2928\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "metadatas_full = [{\"label\": i, \"split\":\"test\"} for i in test_metadata[\"label\"].to_numpy()]\n",
    "n = len(test_metadata[\"id\"])\n",
    "\n",
    "if \"avis_test\" in client.list_collections():\n",
    "    client.delete_collection(\"avis_test\")\n",
    "\n",
    "test_collection = client.get_or_create_collection(\"avis_test\")\n",
    "\n",
    "for i in range(0, n, batch_size):  \n",
    "    ids = test_metadata[\"id\"][i:i+batch_size].tolist()\n",
    "    metadatas = metadatas_full[i:i+batch_size]\n",
    "    documents = X_test['text_clean'][i:i+batch_size].tolist()  \n",
    "    batch_embeddings = test_embedding[i:i+batch_size].tolist()  \n",
    "\n",
    "    test_collection.add(\n",
    "        ids=ids,\n",
    "        embeddings=batch_embeddings,\n",
    "        metadatas=metadatas,\n",
    "        documents=documents  \n",
    "    )\n",
    "    print(f\"Added test batch {i} to {i+len(ids)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
